{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c634ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from config import cfg\n",
    "import argparse\n",
    "from datasets import make_dataloader\n",
    "from model import make_model\n",
    "from processor import do_inference, do_inference_uda\n",
    "from utils.logger import setup_logger\n",
    "from utils.metrics import R1_mAP, R1_mAP_eval, R1_mAP_Pseudo, R1_mAP_query_mining, R1_mAP_save_feature, R1_mAP_draw_figure, Class_accuracy_eval\n",
    "from fair_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f281b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99f0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"ReID Baseline Training\")\n",
    "parser.add_argument(\n",
    "    \"--config_file\", default=\"configs/uda.yml\", help=\"path to config file\", type=str\n",
    ")\n",
    "\n",
    "parser.add_argument(\"opts\", help=\"Modify config options using the command-line\", default=None,\n",
    "                    nargs=argparse.REMAINDER)\n",
    "\n",
    "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "parser.add_argument('-f')    \n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fff4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python test.py --config_file \n",
    "# 'configs/uda.yml' \n",
    "# MODEL.DEVICE_ID \"('0')\" \n",
    "# TEST.WEIGHT \"('../logs/uda/vit_base/visda/transformer_best_model.pth')\" DATASETS.NAMES 'VisDA' DATASETS.NAMES2 'VisDA' OUTPUT_DIR '../logs/uda/vit_base/visda/' DATASETS.ROOT_TRAIN_DIR './data/visda/train/train_image_list.txt' DATASETS.ROOT_TRAIN_DIR2 './data/visda/train/train_image_list.txt' DATASETS.ROOT_TEST_DIR './data/visda/validation/valid_image_list.txt'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89624e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-06 21:12:26,602 reid_baseline INFO: Saving model in the path :./logs/transoct2slo\n",
      "2024-03-06 21:12:26,604 reid_baseline INFO: Namespace(config_file='configs/uda.yml', opts=[], local_rank=0, f='/home/shim/.local/share/jupyter/runtime/kernel-b2e1f3e7-fa03-4c8f-9974-a3e10599a1dc.json')\n",
      "2024-03-06 21:12:26,605 reid_baseline INFO: Loaded configuration file configs/uda.yml\n",
      "2024-03-06 21:12:26,607 reid_baseline INFO: \n",
      "MODEL:\n",
      "  PRETRAIN_CHOICE: 'pretrain'\n",
      "  # PRETRAIN_PATH: './.torch/models/jx_vit_base_p16_224-80ecf9dd.pth'\n",
      "  METRIC_LOSS_TYPE: 'triplet'\n",
      "  IF_LABELSMOOTH: 'off'\n",
      "  IF_WITH_CENTER: 'no'\n",
      "  NAME: 'transformer'\n",
      "  NO_MARGIN: True\n",
      "  DEVICE_ID: ('0')\n",
      "  Transformer_TYPE: 'uda_vit_base_patch16_224_TransReID' #uda_vit_small_patch16_224_TransReID\n",
      "  STRIDE_SIZE: [16, 16]\n",
      "  BLOCK_PATTERN: '3_branches'\n",
      "  TASK_TYPE: 'classify_DA'\n",
      "  UDA_STAGE: 'UDA'\n",
      "#  CAMERA_EMBEDDING: True\n",
      "#  VIEWPOINT_EMBEDDING: True\n",
      "\n",
      "INPUT:\n",
      "  SIZE_TRAIN: [256, 256]\n",
      "  SIZE_TEST: [256, 256]\n",
      "  SIZE_CROP: [224, 224]\n",
      "  PROB: 0.5 # random horizontal flip\n",
      "  RE_PROB: 0.0 # random erasing\n",
      "  PADDING: 0\n",
      "  PIXEL_MEAN: [0.485, 0.456, 0.406]\n",
      "  PIXEL_STD: [0.229, 0.224, 0.225]\n",
      "\n",
      "DATASETS:\n",
      "  NAMES: ('Fundus')\n",
      "  ROOT_TRAIN_DIR: ('./data/FairDomain_Classify/oct_fundus/train/oct_train_list.txt')\n",
      "  NAMES2: ('Fundus')\n",
      "  ROOT_TRAIN_DIR2: ('./data/FairDomain_Classify/slo_fundus/test/slo_test_list.txt')\n",
      "  ROOT_TEST_DIR: ('./data/FairDomain_Classify/slo_fundus/test/slo_test_list.txt')\n",
      "\n",
      "DATALOADER:\n",
      "  SAMPLER: 'softmax'\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 4\n",
      "\n",
      "SOLVER:\n",
      "  OPTIMIZER_NAME: 'SGD'\n",
      "  MAX_EPOCHS: 40\n",
      "  BASE_LR: 0.008\n",
      "  IMS_PER_BATCH: 64\n",
      "  STEPS: [40, 80]\n",
      "  GAMMA: 0.\n",
      "  WARMUP_FACTOR: 0.01\n",
      "  WARMUP_EPOCHS: 10\n",
      "  WARMUP_METHOD: 'linear'\n",
      "  LARGE_FC_LR: False\n",
      "  CHECKPOINT_PERIOD: 120\n",
      "  LOG_PERIOD: 100\n",
      "  EVAL_PERIOD: 1\n",
      "  WEIGHT_DECAY:  1e-4\n",
      "  WEIGHT_DECAY_BIAS: 1e-4\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  WITH_PSEUDO_LABEL_FILTER: True\n",
      "\n",
      "TEST:\n",
      "  EVAL: True\n",
      "  IMS_PER_BATCH: 256\n",
      "  RE_RANKING: False\n",
      "  RE_RANKING_TRACK: False\n",
      "\n",
      "  WEIGHT: './logs/gender/deit_base/fairdomain/oct_fundusslo_fundus/transformer_best_model.pth'\n",
      "  NECK_FEAT: 'after'\n",
      "  FEAT_NORM: 'yes'\n",
      "\n",
      "OUTPUT_DIR: './logs/transoct2slo'\n",
      "\n",
      "\n",
      "\n",
      "2024-03-06 21:12:26,608 reid_baseline INFO: Running with config:\n",
      "DATALOADER:\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 4\n",
      "  SAMPLER: softmax\n",
      "DATASETS:\n",
      "  NAMES: Fundus\n",
      "  NAMES2: Fundus\n",
      "  PLUS_NUM_ID: 100\n",
      "  QUERY_MINING: False\n",
      "  ROOT_TEST_DIR: ./data/FairDomain_Classify/slo_fundus/test/slo_test_list.txt\n",
      "  ROOT_TRAIN_DIR: ./data/FairDomain_Classify/oct_fundus/train/oct_train_list.txt\n",
      "  ROOT_TRAIN_DIR2: ./data/FairDomain_Classify/slo_fundus/test/slo_test_list.txt\n",
      "INPUT:\n",
      "  AA_PROB: 0.0\n",
      "  PADDING: 0\n",
      "  PIXEL_MEAN: [0.485, 0.456, 0.406]\n",
      "  PIXEL_STD: [0.229, 0.224, 0.225]\n",
      "  PROB: 0.5\n",
      "  RE_PROB: 0.0\n",
      "  SIZE_CROP: [224, 224]\n",
      "  SIZE_TEST: [256, 256]\n",
      "  SIZE_TRAIN: [256, 256]\n",
      "MODEL:\n",
      "  AIE_COE: 1.5\n",
      "  BLOCK_PATTERN: 3_branches\n",
      "  CAMERA_EMBEDDING: False\n",
      "  COS_LAYER: False\n",
      "  DEVICE: cuda\n",
      "  DEVICE_ID: 0\n",
      "  DEVIDE_LENGTH: 4\n",
      "  DIST_TRAIN: False\n",
      "  DROP_OUT: 0.0\n",
      "  DROP_PATH: 0.1\n",
      "  FC_SETTING: TransReID\n",
      "  FROZEN: -1\n",
      "  ID_LOSS_TYPE: softmax\n",
      "  ID_LOSS_WEIGHT: 1.0\n",
      "  IF_LABELSMOOTH: off\n",
      "  IF_WITH_CENTER: no\n",
      "  LAST_STRIDE: 1\n",
      "  LOCAL_F: False\n",
      "  METRIC_LOSS_TYPE: triplet\n",
      "  NAME: transformer\n",
      "  NECK: bnneck\n",
      "  NO_MARGIN: True\n",
      "  NO_SHUFFLE: False\n",
      "  PATCH_SHUFFLE: 2\n",
      "  PRETRAIN_CHOICE: pretrain\n",
      "  PRETRAIN_PATH: \n",
      "  PROB: 0.0\n",
      "  RECIP_LOSS: \n",
      "  STRIDE_SIZE: [16, 16]\n",
      "  TASK_TYPE: classify_DA\n",
      "  THRESH: 0.23\n",
      "  TRIPLET_LOSS_WEIGHT: 1.0\n",
      "  Transformer_TYPE: uda_vit_base_patch16_224_TransReID\n",
      "  UDA_STAGE: UDA\n",
      "  VIEWPOINT_EMBEDDING: False\n",
      "  YIWEI_NUM: 5\n",
      "  lameda: 0.5\n",
      "OUTPUT_DIR: ./logs/transoct2slo\n",
      "SOLVER:\n",
      "  BASE_LR: 0.008\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CENTER_LOSS_WEIGHT: 0.0005\n",
      "  CENTER_LR: 0.5\n",
      "  CHECKPOINT_PERIOD: 120\n",
      "  CLUSTER_MARGIN: 0.3\n",
      "  COSINE_MARGIN: 0.5\n",
      "  COSINE_SCALE: 30\n",
      "  EVAL_PERIOD: 1\n",
      "  GAMMA: 0.0\n",
      "  IMS_PER_BATCH: 64\n",
      "  LARGE_FC_LR: False\n",
      "  LOG_PERIOD: 100\n",
      "  MARGIN: 0.3\n",
      "  MAX_EPOCHS: 40\n",
      "  MOMENTUM: 0.9\n",
      "  OPTIMIZER_NAME: SGD\n",
      "  RANGE_ALPHA: 0\n",
      "  RANGE_BETA: 1\n",
      "  RANGE_K: 2\n",
      "  RANGE_LOSS_WEIGHT: 1\n",
      "  RANGE_MARGIN: 0.3\n",
      "  SEED: 1234\n",
      "  STEPS: (40, 80)\n",
      "  WARMUP_EPOCHS: 10\n",
      "  WARMUP_FACTOR: 0.01\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0.0001\n",
      "  WITH_PSEUDO_LABEL_FILTER: True\n",
      "TEST:\n",
      "  DIST_MAT: dist_mat.npy\n",
      "  EVAL: True\n",
      "  FEAT_NORM: yes\n",
      "  FLIP_FEATS: off\n",
      "  IMS_PER_BATCH: 256\n",
      "  NECK_FEAT: after\n",
      "  RE_RANKING: False\n",
      "  RE_RANKING_TRACK: False\n",
      "  WEIGHT: ./logs/gender/deit_base/fairdomain/oct_fundusslo_fundus/transformer_best_model.pth\n"
     ]
    }
   ],
   "source": [
    "if args.config_file != \"\":\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "\n",
    "set_seed(cfg.SOLVER.SEED)\n",
    "if cfg.MODEL.DIST_TRAIN:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "output_dir = cfg.OUTPUT_DIR\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "logger = setup_logger(\"reid_baseline\", output_dir, if_train=True)\n",
    "logger.info(\"Saving model in the path :{}\".format(cfg.OUTPUT_DIR))\n",
    "logger.info(args)\n",
    "\n",
    "if args.config_file != \"\":\n",
    "    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
    "    with open(args.config_file, 'r') as cf:\n",
    "        config_str = \"\\n\" + cf.read()\n",
    "        logger.info(config_str)\n",
    "logger.info(\"Running with config:\\n{}\".format(cfg))\n",
    "\n",
    "if cfg.MODEL.DIST_TRAIN:\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cfg.MODEL.DEVICE_ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9b2fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Fundus loaded\n",
      "Dataset statistics:\n",
      "train oct_train_list and test is slo_test_list\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train   |     2 |     8000 |         1\n",
      "  test    |     2 |     2000 |         1\n",
      "  ----------------------------------------\n",
      "=> Fundus loaded\n",
      "Dataset statistics:\n",
      "train slo_test_list and test is slo_test_list\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train   |     2 |     2000 |         1\n",
      "  test    |     2 |     2000 |         1\n",
      "  ----------------------------------------\n",
      "using Transformer_type: uda_vit_base_patch16_224_TransReID as a backbone\n",
      "using stride: [16, 16], and part number is num_y14 * num_x14\n",
      "using drop_path_rate is : 0.1\n",
      "using aie_xishu is : 1.5\n",
      "using 3branches blocks\n",
      "make model without initialization\n",
      "===========building uda transformer===========\n",
      "Loading pretrained model for finetuning from ./logs/gender/deit_base/fairdomain/oct_fundusslo_fundus/transformer_best_model.pth\n"
     ]
    }
   ],
   "source": [
    "train_loader, train_loader_normal, val_loader, num_query, num_classes, camera_num, view_num, train_loader1, train_loader2, img_num1, img_num2, s_dataset, t_dataset = make_dataloader(cfg)\n",
    "\n",
    "model = make_model(cfg, num_class=num_classes, camera_num=camera_num, view_num=view_num)\n",
    "model.load_param_finetune(cfg.TEST.WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39989552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-06 21:17:26,841 reid_baseline.test INFO: Enter inferencing\n",
      "2024-03-06 21:17:34,827 reid_baseline.test INFO: normal accuracy 0.6315 0.01798652485013008 \n",
      "2024-03-06 21:17:34,828 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model\n",
      "2024-03-06 21:17:34,828 reid_baseline.test INFO: Accuracy: 63.1%\n"
     ]
    }
   ],
   "source": [
    "do_inference_uda(cfg,\n",
    "                 model,\n",
    "                 val_loader,\n",
    "                 num_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c373ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/shim/pyspace/CDTrans/data/FairDomain_Classify'\n",
    "\n",
    "testlist = np.loadtxt('data/FairDomain_Classify/test.txt', dtype=str)\n",
    "\n",
    "imgattr = {}\n",
    "\n",
    "for i, f in enumerate(testlist):\n",
    "    fpath = os.path.join(folder, 'npzs_enhance_224', f)\n",
    "    data = np.load(fpath)\n",
    "    \n",
    "    glaucoma = int(data['glaucoma'])\n",
    "    race = int(data['race'])\n",
    "    gender = int(data['gender'])\n",
    "    ethnicity = int(data['ethnicity'])\n",
    "    language = int(data['language'])\n",
    "    maritalstatus = int(data['marriagestatus'])\n",
    "    attr = [race, gender, ethnicity, language, maritalstatus]\n",
    "    \n",
    "    slofname = 'slo_'+f.split('.')[0]+'.jpg'\n",
    "    \n",
    "    imgattr[slofname] = attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f479cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-06 21:17:44,466 reid_baseline INFO: normal accuracy 0.6315 0.01798652485013008 \n",
      "Classify Domain Adapatation Validation Results - In the source trained model\n",
      "Accuracy: 63.1%\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "if cfg.MODEL.TASK_TYPE == 'classify_DA':\n",
    "    evaluator = Class_accuracy_eval(dataset=cfg.DATASETS.NAMES, logger= logger)\n",
    "elif cfg.TEST.EVAL:\n",
    "    evaluator = R1_mAP_eval(num_query, max_rank=50, feat_norm=cfg.TEST.FEAT_NORM)\n",
    "else:\n",
    "\n",
    "    evaluator = R1_mAP_draw_figure(cfg, num_query, max_rank=50, feat_norm=True,\n",
    "                   reranking=cfg.TEST.RE_RANKING)\n",
    "    # evaluator = R1_mAP_save_feature(num_query, max_rank=50, feat_norm=cfg.TEST.FEAT_NORM,\n",
    "    #                reranking=cfg.TEST.RE_RANKING)\n",
    "evaluator.reset()\n",
    "\n",
    "if device:\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print('Using {} GPUs for inference'.format(torch.cuda.device_count()))\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "model.eval()\n",
    "img_path_list = []\n",
    "pred_all = []\n",
    "gt_all = []\n",
    "att_all = []\n",
    "for n_iter, (img, vid, camid, camids, target_view, img_path) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "#         print(mms)\n",
    "        \n",
    "        img = img.to(device)\n",
    "        camids = camids.to(device)\n",
    "        target_view = target_view.to(device)\n",
    "        target = torch.tensor(vid).to(device)\n",
    "\n",
    "        if cfg.MODEL.TASK_TYPE == 'classify_DA':\n",
    "            probs = model(img, img, cam_label=camids, view_label=target_view, return_logits=True)\n",
    "            \n",
    "            _, predict = torch.max(probs[1], 1)\n",
    "            pred = list(np.array(predict.cpu()))\n",
    "            pred_all.extend(pred)\n",
    "            gt_all.extend(vid)\n",
    "            \n",
    "            # extract attribute\n",
    "            for pa in img_path:\n",
    "                att_all.append(imgattr[pa])\n",
    "            \n",
    "            evaluator.update((probs[1], vid))\n",
    "        else:\n",
    "            feat1, feat2 = model(img, img, cam_label=camids, view_label=target_view, return_logits=False)\n",
    "            evaluator.update((feat2, vid, camid))\n",
    "                \n",
    "if cfg.TEST.EVAL:\n",
    "    if cfg.MODEL.TASK_TYPE == 'classify_DA':\n",
    "        accuracy, mean_ent = evaluator.compute()  \n",
    "        print(\"Classify Domain Adapatation Validation Results - In the source trained model\")\n",
    "        print(\"Accuracy: {:.1%}\".format(accuracy))\n",
    "        \n",
    "    else:\n",
    "        cmc, mAP, _, _, _, _, _ = evaluator.compute()\n",
    "        print(\"Validation Results \")\n",
    "        print(\"mAP: {:.1%}\".format(mAP))\n",
    "        for r in [1, 5, 10]:\n",
    "            print(\"CMC curve, Rank-{:<3}:{:.1%}\".format(r, cmc[r - 1]))\n",
    "        print(cmc[0], cmc[4])\n",
    "else:\n",
    "    print('yes begin saving feature')\n",
    "    feats, distmats, pids, camids, viewids, img_name_path = evaluator.compute()\n",
    "\n",
    "    torch.save(feats, os.path.join(cfg.OUTPUT_DIR, 'features.pth'))\n",
    "    np.save(os.path.join(cfg.OUTPUT_DIR, 'distmat.npy'), distmats)\n",
    "    np.save(os.path.join(cfg.OUTPUT_DIR, 'label.npy'), pids)\n",
    "    np.save(os.path.join(cfg.OUTPUT_DIR, 'camera_label.npy'), camids)\n",
    "    np.save(os.path.join(cfg.OUTPUT_DIR, 'image_name.npy'), img_name_path)\n",
    "    np.save(os.path.join(cfg.OUTPUT_DIR, 'view_label.npy'), viewids)\n",
    "    print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25b33e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000, 2000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_all), len(gt_all), len(att_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98fd20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = np.array(pred_all)\n",
    "gt_all = np.array(gt_all)\n",
    "att_all = np.array(att_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee7b1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# matrix = confusion_matrix(gt_all, pred_all)\n",
    "# acc = matrix.diagonal()/matrix.sum(axis=1) * 100\n",
    "# aacc = acc.mean() / 100\n",
    "# aacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14886eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_acc, esaccs_by_attrs, overall_auc, esaucs_by_attrs, aucs_by_attrs, dpds, eods, between_group_disparity = evalute_comprehensive_perf_scores(pred_all,\n",
    "                                                                                                                        gt_all,\n",
    "                                                                                                                        np.transpose(att_all, (1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1713ff6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6315,\n",
       " array([0.59998761, 0.62068389, 0.6128986 , 0.59950154, 0.45559283]),\n",
       " 0.6361768420967331,\n",
       " array([0.60588572, 0.62942677, 0.60674636, 0.60158973, 0.47124818]),\n",
       " [array([0.66335045, 0.6188599 , 0.63067261]),\n",
       "  array([0.63933482, 0.62861066]),\n",
       "  array([0.63765983, 0.58915441]),\n",
       "  array([0.62083333, 0.63640492, 0.61507937, 0.61535304]),\n",
       "  array([0.76682692, 0.65020356, 0.58852665, 0.63423181, 0.66139241,\n",
       "         0.50568182])],\n",
       " array([0.13327454, 0.04552611, 0.10882968, 0.13274911, 0.26702009]),\n",
       " array([0.11781748, 0.05791791, 0.15179094, 0.08928571, 0.3125    ]),\n",
       " array([[0.0295786 , 0.06993425],\n",
       "        [0.00842859, 0.01685719],\n",
       "        [0.03812259, 0.07624518],\n",
       "        [0.01363366, 0.03352142],\n",
       "        [0.12375071, 0.41049137]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc, esaccs_by_attrs, overall_auc, esaucs_by_attrs, aucs_by_attrs, dpds, eods, between_group_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline\n",
    "race es_auc: 0.6030806561351596 overall_auc: 0.6327946549490443 aucs_by_attrs: [0.65808729 0.61365157 0.62796002] dpds: 0.1299151754336182 eods: 0.10729116368903913 between_group_disparity: [0.02926702 0.0702214 ] \n",
    "gender es_auc: 0.6309805893788262 overall_auc: 0.6327946549490443 aucs_by_attrs: [0.63374986 0.63087487] dpds: 0.034636816929880854 eods: 0.04006863869265742 between_group_disparity: [0.00227166 0.00454333] \n",
    "ethnicity es_auc: 0.5996511132768924 overall_auc: 0.6327946549490443 aucs_by_attrs: [0.63462187 0.57935049] dpds: 0.09312995330304003 eods: 0.14477339181286553 between_group_disparity: [0.04367244 0.08734488] \n",
    "language es_auc: 0.5888229506705296 overall_auc: 0.6327946549490443 aucs_by_attrs: [0.62083333 0.63449165 0.61507937 0.58949097] dpds: 0.10256410256410264 eods: 0.0892857142857143 between_group_disparity: [0.02578184 0.07111419] \n",
    "maritalstatus es_auc: 0.45490739806218333 overall_auc: 0.6327946549490443 aucs_by_attrs: [0.76682692 0.64716096 0.58642581 0.63423181 0.65506329 0.46022727] dpds: 0.2605698529411765 eods: 0.3125 between_group_disparity: [0.144596   0.48451681] \n",
    "\n",
    "                        \n",
    "# race\n",
    "race es_auc: 0.6018786968218406 overall_auc: 0.6349091550001975 aucs_by_attrs: [0.65641849 0.6032349  0.63321381] dpds: 0.12779565268640236 eods: 0.1283437952679865 between_group_disparity: [0.03428953 0.08376566] \n",
    "gender es_auc: 0.6332105633427468 overall_auc: 0.6349091550001975 aucs_by_attrs: [0.63582157 0.63313907] dpds: 0.03686001610723466 eods: 0.04216949587446178 between_group_disparity: [0.00211251 0.00422502] \n",
    "ethnicity es_auc: 0.5970130148111232 overall_auc: 0.6349091550001975 aucs_by_attrs: [0.63700565 0.57352941] dpds: 0.10522842543884459 eods: 0.16666666666666663 between_group_disparity: [0.04998844 0.09997688] \n",
    "language es_auc: 0.5957686044755565 overall_auc: 0.6349091550001975 aucs_by_attrs: [0.62083333 0.63647176 0.61507937 0.6046798 ] dpds: 0.1102299762093577 eods: 0.0892857142857143 between_group_disparity: [0.01810906 0.05007324] \n",
    "maritalstatus es_auc: 0.4638534459975093 overall_auc: 0.6349091550001975 aucs_by_attrs: [0.76682692 0.64857221 0.59190026 0.63908356 0.63623418 0.46022727] dpds: 0.2664292279411765 eods: 0.3125 between_group_disparity: [0.14267154 0.48290318] \n",
    "                        \n",
    "# gender                         \n",
    "\n",
    "\n",
    "# ethnicity\n",
    "\n",
    "race es_auc: 0.6058857187101729 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.66335045 0.6188599  0.63067261] dpds: 0.133274542589092 eods: 0.11781747947851284 between_group_disparity: [0.0295786  0.06993425] \n",
    "gender es_auc: 0.6294267740446075 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.63933482 0.62861066] dpds: 0.04552610912588817 eods: 0.057917906199132396 between_group_disparity: [0.00842859 0.01685719] \n",
    "ethnicity es_auc: 0.6067463577471336 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.63765983 0.58915441] dpds: 0.10882968493693079 eods: 0.15179093567251467 between_group_disparity: [0.03812259 0.07624518] \n",
    "language es_auc: 0.6015897257570374 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.62083333 0.63640492 0.61507937 0.61535304] dpds: 0.13274910743265178 eods: 0.0892857142857143 between_group_disparity: [0.01363366 0.03352142] \n",
    "maritalstatus es_auc: 0.47124817509444583 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.76682692 0.65020356 0.58852665 0.63423181 0.66139241 0.50568182] dpds: 0.2670200892857143 eods: 0.3125 between_group_disparity: [0.12375071 0.41049137] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a32a721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race es_auc: 0.6058857187101729 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.66335045 0.6188599  0.63067261] dpds: 0.133274542589092 eods: 0.11781747947851284 between_group_disparity: [0.0295786  0.06993425] \n",
      "\n",
      "gender es_auc: 0.6294267740446075 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.63933482 0.62861066] dpds: 0.04552610912588817 eods: 0.057917906199132396 between_group_disparity: [0.00842859 0.01685719] \n",
      "\n",
      "ethnicity es_auc: 0.6067463577471336 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.63765983 0.58915441] dpds: 0.10882968493693079 eods: 0.15179093567251467 between_group_disparity: [0.03812259 0.07624518] \n",
      "\n",
      "language es_auc: 0.6015897257570374 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.62083333 0.63640492 0.61507937 0.61535304] dpds: 0.13274910743265178 eods: 0.0892857142857143 between_group_disparity: [0.01363366 0.03352142] \n",
      "\n",
      "maritalstatus es_auc: 0.47124817509444583 overall_auc: 0.6361768420967331 aucs_by_attrs: [0.76682692 0.65020356 0.58852665 0.63423181 0.66139241 0.50568182] dpds: 0.2670200892857143 eods: 0.3125 between_group_disparity: [0.12375071 0.41049137] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "attrs = ['race', 'gender', 'ethnicity', 'language', 'maritalstatus']\n",
    "for i, attr in enumerate(attrs):\n",
    "    print(attr, f'es_auc: {esaucs_by_attrs[i]}', f'overall_auc: {overall_auc}', f'aucs_by_attrs: {aucs_by_attrs[i]}',\n",
    "         f'dpds: {dpds[i]}', f'eods: {eods[i]}', f'between_group_disparity: {between_group_disparity[i]}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7ce4de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03825089118445411 0.09309112395227297\n",
      "0.0016826342556808332 0.0033652685113616664\n",
      "0.045764221217604914 0.09152844243520983\n"
     ]
    }
   ],
   "source": [
    "auc = 0.6327946549\n",
    "group_aucs_race = [0.67913992, 0.61897936, 0.6431148]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6327946549\n",
    "group_aucs_race = [0.64704352, 0.6448687]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6327946549\n",
    "group_aucs_race = [0.64830508, 0.58915441]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4c37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c974e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008667084860357523 0.020708440258146586\n",
      "0.0024680847572456322 0.0049361695144912645\n",
      "0.0443468337211254 0.0886936674422508\n"
     ]
    }
   ],
   "source": [
    "auc = 0.7615\n",
    "group_aucs_race = [0.7673940949935816, 0.751624617737003, 0.7564252346931267]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.7615\n",
    "group_aucs_race = [0.7632310259898939, 0.7594721329046088]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.7615\n",
    "group_aucs_race = [0.7630059140317837, 0.6954656862745098]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5ad432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005386185093454525 0.013193392677923802\n",
      "0.005559610099387723 0.011119220198775447\n",
      "0.08855632325191373 0.17711264650382746\n"
     ]
    }
   ],
   "source": [
    "auc = 0.6724\n",
    "group_aucs_race = [0.6681643132220796, 0.6770355504587156, 0.6726104910654197]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6724\n",
    "group_aucs_race = [0.6705647010757496, 0.6780412647374062]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6724\n",
    "group_aucs_race = [0.6766885827248599, 0.5575980392156863]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c12629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009224104155145327 0.020727805937435062\n",
      "0.008686211180702142 0.017372422361404283\n",
      "0.09480312197065499 0.18960624394130998\n"
     ]
    }
   ],
   "source": [
    "auc = 0.6963\n",
    "group_aucs_race = [0.6785622593068037, 0.6929950305810397, 0.6803561269666935]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6754\n",
    "group_aucs_race = [0.6722162908031312, 0.6839496248660236]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6754\n",
    "group_aucs_race = [0.6801433904912941, 0.5520833333333334]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c40699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01635959581708581 0.04002276811752837\n",
      "0.006063135744122517 0.012126271488245034\n",
      "0.07414753495020845 0.1482950699004169\n"
     ]
    }
   ],
   "source": [
    "auc = 0.6758\n",
    "group_aucs_race = [0.7037227214377406, 0.6890290519877675, 0.676675334743915]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6758\n",
    "group_aucs_race = [0.6783851807883691, 0.6701902465166131]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)\n",
    "\n",
    "auc = 0.6758\n",
    "group_aucs_race = [0.6798746709837997, 0.579656862745098]\n",
    "mean_disparity = float(np.std(group_aucs_race) / (auc))\n",
    "max_disparity = (np.max(group_aucs_race)-np.min(group_aucs_race)) / np.abs(auc)\n",
    "print(mean_disparity, max_disparity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
